"""
CLI entry point: python -m nsys_tui <command> [options]

Profile: path to .sqlite or .nsys-rep (open/analyze/etc accept both; .nsys-rep is converted when nsys is on PATH).

Commands:
    info       <profile>                            Show GPU hardware and profile metadata
    analyze    <profile> --gpu N --trim S E         Full auto-report (bottlenecks, overlap, iters, NVTX)
    summary    <profile> [--gpu N]                  GPU kernel summary with top kernels
    overlap    <profile> --gpu N --trim S E         Compute/NCCL overlap analysis
    nccl       <profile> --gpu N --trim S E         NCCL collective breakdown
    iters      <profile> --gpu N --trim S E         Detect training iterations
    tree       <profile> --gpu N --trim S E         NVTX hierarchy as text
    markdown   <profile> --gpu N --trim S E         NVTX hierarchy as markdown
    search     <profile> --query Q                  Search kernels/NVTX by name
    export-csv <profile> --gpu N --trim S E         Export flat CSV
    export-json <profile> --gpu N --trim S E       Export flat JSON
    export     <profile> [--gpu N] -o DIR           Export Perfetto JSON traces
    viewer     <profile> --gpu N --trim S E -o FILE Generate interactive HTML viewer
    timeline-html <profile> --gpu N --trim S E -o FILE Generate horizontal timeline HTML
    web        <profile> --gpu N --trim S E        Serve viewer in browser (local HTTP)
    open       <profile> [--gpu N] [--trim S E]     One-click: Perfetto / web / TUI (auto .sqlite|.nsys-rep)
    perfetto   <profile> --gpu N --trim S E        Open in Perfetto UI (via local trace server)
    timeline-web <profile> --gpu N --trim S E      Horizontal timeline in browser
    tui        <profile> --gpu N --trim S E        Terminal tree view
    timeline   <profile> --gpu N --trim S E        Horizontal timeline (Perfetto-style)
"""
import sys
import os
import argparse


def _add_gpu_trim(p, gpu_required=True, trim_required=True):
    """Add standard --gpu and --trim arguments to a subparser."""
    p.add_argument("profile", help="Path to profile (.sqlite or .nsys-rep)")
    p.add_argument("--gpu", type=int, required=gpu_required,
                   default=None, help="GPU device ID")
    p.add_argument("--trim", nargs=2, type=float,
                   required=trim_required,
                   metavar=("START_S", "END_S"),
                   help="Time window in seconds")


def _parse_trim(args):
    """Convert --trim seconds to nanoseconds tuple, or None."""
    if args.trim:
        return (int(args.trim[0] * 1e9), int(args.trim[1] * 1e9))
    return None


def main():
    parser = argparse.ArgumentParser(
        prog="nsys-ai",
        description="Terminal UI for NVIDIA Nsight Systems profiles",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__,
    )
    sub = parser.add_subparsers(dest="command")

    # ── info ──
    p = sub.add_parser("info", help="Show profile metadata and GPU info")
    p.add_argument("profile", help="Path to profile (.sqlite or .nsys-rep)")

    # ── analyze ──
    p = sub.add_parser("analyze", help="Full auto-report: bottlenecks, overlap, iters, NVTX hierarchy")
    _add_gpu_trim(p)
    p.add_argument("-o", "--output", default=None, help="Write markdown report to file (default: terminal only)")

    # ── summary ──
    p = sub.add_parser("summary", help="GPU kernel summary with top kernels")
    _add_gpu_trim(p, gpu_required=False, trim_required=False)

    # ── overlap ──
    p = sub.add_parser("overlap", help="Compute/NCCL overlap analysis")
    _add_gpu_trim(p)

    # ── nccl ──
    p = sub.add_parser("nccl", help="NCCL collective breakdown")
    _add_gpu_trim(p)

    # ── iters ──
    p = sub.add_parser("iters", help="Detect training iterations")
    _add_gpu_trim(p)

    # ── tree ──
    p = sub.add_parser("tree", help="NVTX hierarchy as text")
    _add_gpu_trim(p)

    # ── markdown ──
    p = sub.add_parser("markdown", help="NVTX hierarchy as markdown")
    _add_gpu_trim(p)

    # ── search ──
    p = sub.add_parser("search", help="Search kernels/NVTX by name")
    p.add_argument("profile", help="Path to profile (.sqlite or .nsys-rep)")
    p.add_argument("--query", "-q", required=True, help="Search query (substring)")
    p.add_argument("--gpu", type=int, default=None, help="GPU device ID")
    p.add_argument("--trim", nargs=2, type=float, metavar=("START_S", "END_S"),
                   help="Time window in seconds")
    p.add_argument("--parent", default=None, help="NVTX parent pattern for hierarchical search")
    p.add_argument("--type", choices=["kernel", "nvtx", "hierarchy"],
                   default="kernel", help="Search type (default: kernel)")
    p.add_argument("--limit", type=int, default=200, help="Max results")

    # ── export-csv ──
    p = sub.add_parser("export-csv", help="Export kernel data as flat CSV")
    _add_gpu_trim(p)
    p.add_argument("-o", "--output", default=None, help="Output file (default: stdout)")

    # ── export-json ──
    p = sub.add_parser("export-json", help="Export kernel data as flat JSON")
    _add_gpu_trim(p)
    p.add_argument("-o", "--output", default=None, help="Output file (default: stdout)")
    p.add_argument("--summary", action="store_true", help="Export summary instead of flat list")

    # ── export ──
    p = sub.add_parser("export", help="Export Perfetto JSON traces")
    _add_gpu_trim(p, gpu_required=False)
    p.add_argument("-o", "--output", default=".", help="Output directory")

    # ── viewer ──
    p = sub.add_parser("viewer", help="Generate interactive HTML viewer")
    _add_gpu_trim(p)
    p.add_argument("-o", "--output", default="nvtx_tree.html", help="Output HTML file")

    # ── timeline-html ──
    p = sub.add_parser("timeline-html", help="Generate horizontal timeline HTML")
    _add_gpu_trim(p)
    p.add_argument("-o", "--output", default="timeline.html", help="Output HTML file")

    # ── web ──
    p = sub.add_parser("web", help="Serve interactive viewer in browser")
    _add_gpu_trim(p)
    p.add_argument("--port", type=int, default=8142, help="HTTP port (default: 8142)")
    p.add_argument("--no-browser", action="store_true", help="Don't auto-open browser")

    # ── open ──
    p = sub.add_parser("open", help="One-click open: pick Perfetto, web viewer, or TUI (auto .sqlite/.nsys-rep)")
    p.add_argument("profile", help="Path to profile (.sqlite or .nsys-rep)")
    p.add_argument("--gpu", type=int, default=None, help="GPU device ID (default: first GPU in profile)")
    p.add_argument("--trim", nargs=2, type=float, metavar=("START_S", "END_S"), default=None,
                   help="Time window in seconds (default: full profile)")
    p.add_argument("--viewer", choices=["perfetto", "web", "tui"], default="perfetto",
                   help="Viewer to use (default: perfetto)")
    p.add_argument("--port", type=int, default=None, help="HTTP port for perfetto/web (default: 8143/8142)")
    p.add_argument("--no-browser", action="store_true", help="Don't auto-open browser (perfetto/web)")

    # ── perfetto ──
    p = sub.add_parser("perfetto", help="Open trace in Perfetto UI")
    _add_gpu_trim(p)
    p.add_argument("--port", type=int, default=8143, help="HTTP port for trace (default: 8143)")
    p.add_argument("--no-browser", action="store_true", help="Don't auto-open browser")

    # ── timeline-web ──
    p = sub.add_parser("timeline-web", help="Horizontal timeline in browser")
    _add_gpu_trim(p)
    p.add_argument("--port", type=int, default=8144, help="HTTP port (default: 8144)")
    p.add_argument("--no-browser", action="store_true", help="Don't auto-open browser")

    # ── tui ──
    p = sub.add_parser("tui", help="Terminal tree view (rich)")
    _add_gpu_trim(p)
    p.add_argument("--depth", type=int, default=-1, help="Max tree depth (-1=all)")
    p.add_argument("--min-ms", type=float, default=0, help="Min duration to show (ms)")

    # ── timeline ──
    p = sub.add_parser("timeline", help="Horizontal timeline view (Perfetto-style)")
    _add_gpu_trim(p)
    p.add_argument("--min-ms", type=float, default=0, help="Min duration to show (ms)")

    # ── skill ──
    p = sub.add_parser("skill", help="List or run analysis skills")
    skill_sub = p.add_subparsers(dest="skill_action")
    skill_sub.add_parser("list", help="List all available skills")
    sp_run = skill_sub.add_parser("run", help="Run a skill against a profile")
    sp_run.add_argument("skill_name", help="Name of the skill to run")
    sp_run.add_argument("profile", help="Path to .sqlite file")

    # ── agent ──
    p = sub.add_parser("agent", help="AI agent for profile analysis")
    agent_sub = p.add_subparsers(dest="agent_action")
    sp_analyze = agent_sub.add_parser("analyze", help="Full auto-analysis report")
    sp_analyze.add_argument("profile", help="Path to .sqlite file")
    sp_ask = agent_sub.add_parser("ask", help="Ask a question about a profile")
    sp_ask.add_argument("profile", help="Path to .sqlite file")
    sp_ask.add_argument("question", help="Natural language question")

    # ── help ──
    sub.add_parser("help", help="Show getting-started guide and available commands")

    args = parser.parse_args()
    if not args.command:
        # No args → launch interactive TUI main page
        from .main_page import run_main_page
        run_main_page()
        return

    if args.command == "help":
        from .main_page import show_help
        show_help()
        return

    # Import here to avoid slow startup for --help
    from . import profile as _profile

    try:
        if args.command == "analyze":
            from .report import run_analyze, format_report_terminal, format_report_markdown
            prof = _profile.open(args.profile)
            try:
                trim = _parse_trim(args)
                data = run_analyze(prof, args.gpu, trim)
                print(format_report_terminal(data))
                if args.output:
                    md = format_report_markdown(data, args.profile, trim)
                    out_path = args.output
                    out_dir = os.path.dirname(out_path)
                    if out_dir:
                        os.makedirs(out_dir, exist_ok=True)
                    with open(out_path, "w", encoding="utf-8", newline="\n") as f:
                        f.write(md)
                    print(f"Markdown report written to {out_path}")
            finally:
                prof.close()

        elif args.command == "info":
            prof = _profile.open(args.profile)
            m = prof.meta
            print(f"Profile: {args.profile}")
            if getattr(prof, "schema", None) and getattr(prof.schema, "version", None):
                print(f"  Nsight version (heuristic): {prof.schema.version}")
            print(f"  GPUs: {m.devices}")
            print(f"  Kernels: {m.kernel_count}  |  NVTX: {m.nvtx_count}")
            print(f"  Time: {m.time_range[0]/1e9:.3f}s – {m.time_range[1]/1e9:.3f}s")
            print()
            for dev, info in m.gpu_info.items():
                print(f"  GPU {dev}: {info.name} | PCI={info.pci_bus} | "
                      f"SMs={info.sm_count} | Mem={info.memory_bytes/1e9:.0f}GB | "
                      f"Kernels={info.kernel_count} | Streams={info.streams}")
            prof.close()

        elif args.command == "summary":
            from .summary import gpu_summary, format_text, auto_commentary
            prof = _profile.open(args.profile)
            trim = _parse_trim(args)
            gpus = [args.gpu] if args.gpu is not None else prof.meta.devices
            for gpu in gpus:
                s = gpu_summary(prof, gpu, trim)
                print(format_text(s))
                print()
                print(auto_commentary(s))
                print()
            prof.close()

        elif args.command == "overlap":
            from .overlap import overlap_analysis, format_overlap
            prof = _profile.open(args.profile)
            print(format_overlap(overlap_analysis(prof, args.gpu, _parse_trim(args))))
            prof.close()

        elif args.command == "nccl":
            from .overlap import nccl_breakdown, format_nccl
            prof = _profile.open(args.profile)
            print(format_nccl(nccl_breakdown(prof, args.gpu, _parse_trim(args))))
            prof.close()

        elif args.command == "iters":
            from .overlap import detect_iterations, format_iterations
            prof = _profile.open(args.profile)
            print(format_iterations(detect_iterations(prof, args.gpu, _parse_trim(args))))
            prof.close()

        elif args.command == "tree":
            from .tree import build_nvtx_tree, format_text
            prof = _profile.open(args.profile)
            roots = build_nvtx_tree(prof, args.gpu, _parse_trim(args))
            print(format_text(roots))
            prof.close()

        elif args.command == "markdown":
            from .tree import build_nvtx_tree, format_markdown
            prof = _profile.open(args.profile)
            roots = build_nvtx_tree(prof, args.gpu, _parse_trim(args))
            print(format_markdown(roots))
            prof.close()

        elif args.command == "export":
            from . import export
            prof = _profile.open(args.profile)
            try:
                trim = _parse_trim(args)
                os.makedirs(args.output, exist_ok=True)
                gpus = [args.gpu] if args.gpu is not None else prof.meta.devices
                for gpu in gpus:
                    events = export.gpu_trace(prof, gpu, trim)
                    if not events:
                        print(f"GPU {gpu}: no kernels, skipped")
                        continue
                    out = os.path.join(args.output, f"trace_gpu{gpu}.json")
                    export.write_json(events, out)
                    nk = sum(1 for e in events if e.get("cat") == "gpu_kernel")
                    nn = sum(1 for e in events if e.get("cat") == "nvtx_projected")
                    print(f"GPU {gpu}: {nk} kernels, {nn} NVTX → {out}")
            finally:
                prof.close()

        elif args.command == "search":
            from .search import (search_kernels, search_nvtx,
                                 search_hierarchy, format_results)
            prof = _profile.open(args.profile)
            trim = _parse_trim(args)

            if args.parent or args.type == "hierarchy":
                if not args.gpu or not trim:
                    print("Error: hierarchical search requires --gpu and --trim")
                    prof.close()
                    return
                results = search_hierarchy(prof, args.parent or "", args.query,
                                           args.gpu, trim)
                print(format_results(results, "hierarchy"))
            elif args.type == "nvtx":
                results = search_nvtx(prof, args.query, args.gpu, trim, args.limit)
                print(format_results(results, "nvtx"))
            else:
                results = search_kernels(prof, args.query, args.gpu, trim, args.limit)
                print(format_results(results, "kernel"))
            prof.close()

        elif args.command == "export-csv":
            from .export_flat import to_csv
            prof = _profile.open(args.profile)
            trim = _parse_trim(args)
            content = to_csv(prof, args.gpu, trim, args.output)
            if not args.output:
                print(content)
            else:
                print(f"CSV written to {args.output}")
            prof.close()

        elif args.command == "export-json":
            import json as _json
            from .export_flat import to_json_flat, to_summary_json
            prof = _profile.open(args.profile)
            trim = _parse_trim(args)
            if args.summary:
                data = to_summary_json(prof, args.gpu, trim, args.output)
            else:
                data = to_json_flat(prof, args.gpu, trim, args.output)
            if not args.output:
                print(_json.dumps(data, indent=2))
            else:
                print(f"JSON written to {args.output}")
            prof.close()

        elif args.command == "viewer":
            from .viewer import write_html
            prof = _profile.open(args.profile)
            write_html(prof, args.gpu, _parse_trim(args), args.output)
            print(f"Written to {args.output} ({os.path.getsize(args.output)//1024} KB)")
            prof.close()

        elif args.command == "timeline-html":
            from .viewer import write_timeline_html
            prof = _profile.open(args.profile)
            write_timeline_html(prof, args.gpu, _parse_trim(args), args.output)
            print(f"Written to {args.output} ({os.path.getsize(args.output)//1024} KB)")
            prof.close()

        elif args.command == "open":
            from .web import serve, serve_perfetto
            from .tui import run_tui
            prof = _profile.open(args.profile)
            gpu = args.gpu if args.gpu is not None else (prof.meta.devices[0] if prof.meta.devices else 0)
            if args.trim:
                trim_ns = (int(args.trim[0] * 1e9), int(args.trim[1] * 1e9))
            else:
                trim_ns = (int(prof.meta.time_range[0]), int(prof.meta.time_range[1]))
            port = args.port if args.port is not None else (8143 if args.viewer == "perfetto" else 8142)
            if args.viewer == "perfetto":
                serve_perfetto(prof, gpu, trim_ns, port=port, open_browser=not args.no_browser)
            elif args.viewer == "web":
                serve(prof, gpu, trim_ns, port=port, open_browser=not args.no_browser)
            else:
                prof.close()
                run_tui(prof.path, gpu, trim_ns, max_depth=-1, min_ms=0)

        elif args.command == "web":
            from .web import serve
            prof = _profile.open(args.profile)
            serve(prof, args.gpu, _parse_trim(args),
                  port=args.port, open_browser=not args.no_browser)

        elif args.command == "perfetto":
            from .web import serve_perfetto
            prof = _profile.open(args.profile)
            serve_perfetto(prof, args.gpu, _parse_trim(args),
                           port=args.port, open_browser=not args.no_browser)

        elif args.command == "timeline-web":
            from .web import serve_timeline
            prof = _profile.open(args.profile)
            serve_timeline(prof, args.gpu, _parse_trim(args),
                           port=args.port, open_browser=not args.no_browser)

        elif args.command == "tui":
            from .tui import run_tui
            run_tui(args.profile, args.gpu, _parse_trim(args),
                    max_depth=args.depth, min_ms=args.min_ms)

        elif args.command == "timeline":
            from .tui_timeline import run_timeline
            run_timeline(args.profile, args.gpu, _parse_trim(args),
                         min_ms=args.min_ms)
    except RuntimeError as e:
        # Surface schema/validation issues without a full Python traceback.
        print(f"Error: {e}")

    elif args.command == "skill":
        from .skills.registry import list_skills, all_skills, run_skill as _run_skill
        if args.skill_action == "list":
            skills = all_skills()
            print(f"{'Name':<25s}  {'Category':<15s}  Description")
            print("─" * 80)
            for s in skills:
                print(f"{s.name:<25s}  {s.category:<15s}  {s.description[:60]}")
        elif args.skill_action == "run":
            import sqlite3
            conn = sqlite3.connect(args.profile)
            try:
                print(_run_skill(args.skill_name, conn))
            finally:
                conn.close()
        else:
            parser.parse_args(["skill", "--help"])

    elif args.command == "agent":
        from .agent.loop import Agent
        if args.agent_action == "analyze":
            agent = Agent(args.profile)
            try:
                print(agent.analyze())
            finally:
                agent.close()
        elif args.agent_action == "ask":
            agent = Agent(args.profile)
            try:
                print(agent.ask(args.question))
            finally:
                agent.close()
        else:
            parser.parse_args(["agent", "--help"])


if __name__ == "__main__":
    main()
